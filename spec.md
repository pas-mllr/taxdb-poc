"""
\########################################################################################################

# Improved Machineâ€‘Executable Specification â€“Â TaxDBâ€‘POC (Belgiumâ€¯|â€¯Spainâ€¯|â€¯Germany)

# v2 â€“Â Tweaks for reliability, local dev ergonomics, and costâ€‘control

\########################################################################################################

### ðŸ“œ Instruction Prompt  (place at the very top of **README.md**)

> **You are an AI coding assistant. Implement every item marked â€œ`REQ:`â€ in this document.**
> **Absolutely no deviations** from directory layout, file names, or versions.
>
> 1. Scaffold the repo exactly as described.
> 2. Use **PythonÂ 3.12** and pin all thirdâ€‘party packages *exactly* to the versions stated.
> 3. Provision Azure resources via **Bicep** â€“Â the template must deploy endâ€‘toâ€‘end with a single CLI command.
> 4. Provide a frictionâ€‘free **local mode** (Docker Compose: Postgresâ€¯+â€¯Azurite) so that `make test` does **not** require an Azure subscription or paid OpenAI credits.
> 5. Keep the entire POCâ€™s recurring cloud spend **â‰¤â€¯â‚¬5â€¯/â€¯month** â€“ this means *Basic* tier everywhere and consumptionâ€‘plan Functions.
> 6. CI must finish in **<â€¯15Â min** and never hit a paid Azure/OpenAI endpoint.

---

## 0. ðŸŒ¡ï¸ DefinitionÂ ofÂ Done (`make test`)

`make test` (run on GitHub Actions and locally) **MUST**:

1. **Download â‰¥â€¯1 document** per jurisdiction published â‰¤â€¯24â€¯h ago *or* gracefully skip if none available, recording â€œ0Â rows fetchedâ€ without raising.
2. **Insert** each document into the local Postgres (with pgvector) **and** a *local* search shim.
3. **Expose** them via the FastAPI service:

   * `GET /healthz` returns `{"status":"ok"}` (`200`).
   * `GET /search?q=tax&jurisdiction=ES` returns â‰¥â€¯1 hit (unless 0 docs ingested â€“ then an empty list, still `200`).
   * `GET /doc/{id}` streams JSON metadata and a presigned blob URL.
4. **All tests green** (`pytest -q` exitsÂ 0).

Cloud deployment is validated separately by `make deploy-infra && make smoke-cloud`.

---

## 1. ðŸ”’ VersionÂ MatrixÂ (immutable)

| Layer                  | Version      |
| ---------------------- | ------------ |
| Python                 | **3.12.2**   |
| FastAPI                | **0.111.0**  |
| SQLAlchemy             | **2.0.29**   |
| pgvectorâ€‘python        | **0.2.5**    |
| azureâ€‘identity         | **1.16.0**   |
| azureâ€‘storageâ€‘blob     | **12.19.1**  |
| azureâ€‘searchâ€‘documents | **11.5.0b7** |
| azureâ€‘aiâ€‘generative    | **1.0.0b1**  |
| pdfminer.six           | **20221105** |
| pytesseract            | **0.3.12**   |
| pytest                 | **8.2.1**    |
| httpx                  | **0.27.0**   |
| Bicep                  | **0.23.2**   |

*`REQ:` Freeze these in `pyproject.toml`Â â†’Â `poetry.lock`.*

---

## 2. DirectoryÂ LayoutÂ (ðŸ’¥ **MUST match exactly**)Â `REQ:`

```
.
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ main.bicep
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ pgvector-init.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ etl/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ utils.py
â”‚       â”œâ”€â”€ be_moniteur.py
â”‚       â”œâ”€â”€ es_boe.py
â”‚       â””â”€â”€ de_bgbl.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ gen_openapi.py
â”‚   â””â”€â”€ load_sample.sh
â”œâ”€â”€ openapi/
â”‚   â””â”€â”€ taxdb.yaml
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_e2e.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md   <-- this file
```

---

## 3. EnvironmentÂ VariablesÂ `.env`Â `REQ:`

```
# local mode (false = cloud)
LOCAL_MODE=true

# Azure / OpenAI â€“ leave blank for local runs
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_KEY=
AZURE_SEARCH_ENDPOINT=
AZURE_SEARCH_KEY=
AZURE_PG_CONNSTR=
AZURE_BLOB_CONNSTR=

# CI convenience
DOC_LOOKBACK_HOURS=48
```

*If `LOCAL_MODE=true`, all Azure/OpenAI calls **MUST** be stubbed or routed to local shims so tests never hit the internet.*

---

## 4. DataÂ ModelÂ `src/models.py`Â `REQ:`

```python
from sqlalchemy import (
    Column, Date, DateTime, Enum, Float, String, Text, func, Index
)
from pgvector.sqlalchemy import Vector
from sqlalchemy.orm import DeclarativeBase

class Base(DeclarativeBase): pass

class Document(Base):
    __tablename__ = "documents"

    id            = Column(String, primary_key=True)  # "BE:20250804:AR-123"
    jurisdiction  = Column(Enum("BE", "ES", "DE", name="jurisd"))
    source_system = Column(String, nullable=False)
    document_type = Column(String, nullable=False)
    title         = Column(Text, nullable=False)
    summary       = Column(Text, nullable=True)
    issue_date    = Column(Date, nullable=False)
    effective_date= Column(Date, nullable=True)
    language_orig = Column(String(2), nullable=False)
    blob_url      = Column(Text, nullable=False)
    checksum      = Column(String(64), unique=True, nullable=False)
    vector        = Column(Vector(1536))
    created_at    = Column(DateTime, server_default=func.now())

    __table_args__ = (
        Index("ix_jurisdiction_date", "jurisdiction", "issue_date"),
        Index("ix_vector", "vector", postgresql_using="ivfflat"),
    )
```

---

## 5. InfrastructureÂ asÂ CodeÂ `infra/main.bicep`Â `REQ:`

* Resourceâ€¯Group param `rgName`
* Deploy:

  * **StorageÂ Account** â€“ containers `raw` & `parsed` (lifecycle rule: delete after 30Â days).
  * **PostgreSQL FlexibleÂ Server** (Basicâ€¯SKU) + `pgvector` extension.
  * **Azureâ€¯AIÂ Search** (Basic) â€“ index `documents`.
  * **FunctionÂ App** (Python, Consumption â€‘ Linux).
  * **KeyÂ Vault** â€“ store all secrets.
* Outputs: `PG_CONNSTR`, `BLOB_CONNSTR`, `SEARCH_ENDPOINT`, `FUNCTION_APP_URL`.
* **Cost guardâ€‘rail** â€“Â set throughput to the minimum allowed (AIÂ SearchÂ =Â 1 replicaâ€¯Ã—â€¯1 partition).

---

## 6. ETL FrameworkÂ `src/etl/utils.py`Â `REQ:`

* Implement **one generic** async function `run_pipeline(jurisdiction: str, fetch: Callable)` used by all three loaders.
* Provide two textâ€‘embedding strategies:

  * `AzureOpenAIEmbedding` (used when `LOCAL_MODE=false`).
  * `LocalMiniLMEmbedding` (HuggingFace `allâ€‘MiniLMâ€‘L6â€v2`) for local/CI.
* **Download cache** (`~/.cache/taxdb/`) so repeated tests avoid network.
* **Idempotency**: skip insert if `checksum` already exists.

### Jurisdictionâ€‘specific loadersÂ `REQ:`

| File                | Fetch logic                                                                    | Parse logic                                       | Min. fields                    |
| ------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------- | ------------------------------ |
| **be\_moniteur.py** | GET XML: `https://www.ejustice.just.fgov.be/eli/{YYYY}{MM}{DD}/MONITOR/nl/xml` | XPath on `<doc>` nodes                            | `id`Â (eli), title, issue\_date |
| **es\_boe.py**      | GET XML: `https://www.boe.es/diario_boe/xml.php?id=BOE-B-{YYYY}{MM}{DD}`       | Only `<item>` where `<materia>` contains `FISCAL` | same                           |
| **de\_bgbl.py**     | Find todayâ€™s PDF via index JSON `https://www.bgbl.de/metadata.aktion?today`    | `pdfminer.six` â†’ text; languageÂ =Â â€œdeâ€            | same                           |

*All three loaders must call `run_pipeline`.*

---

## 7. API ServiceÂ `src/api.py`Â `REQ:`

* FastAPI app factory pattern (`create_app(settings)`).
* **/search** â€“ hybrid:

  1. IF `LOCAL_MODE=true` â†’ cosine similarity via pgvectorâ€¯+â€¯ILIKE.
  2. ELSE â†’ AzureÂ AIÂ Search hybrid query (vector + BM25).
* **/doc/{id}** â€“Â returns Pydantic DTO + **SAS URL** valid 15â€¯min.
* **CORS** â€“ allow originsÂ `https://*.shell.com` + `http://localhost:*`.

---

## 8. LocalÂ DevÂ /Â CIÂ DockerÂ ComposeÂ `docker/docker-compose.yml`Â `REQ:`

```yaml
version: "3.9"
services:
  db:
    image: ankane/pgvector:0.5.1
    environment:
      POSTGRES_USER: taxdb
      POSTGRES_PASSWORD: taxdb
      POSTGRES_DB: taxdb
    ports: ["5432:5432"]
    volumes: ["./pgvector-init.sql:/docker-entrypoint-initdb.d/pgvector.sql:ro"]

  azurite:
    image: mcr.microsoft.com/azure-storage/azurite:3.29.0
    command: "azurite-blob --blobHost 0.0.0.0 -l /data"
    ports: ["10000:10000"]
```

*`pgvector-init.sql` simply: `CREATE EXTENSION IF NOT EXISTS pgvector;`*

---

## 9. MakefileÂ `REQ:`

```make
PY?=python3.12

init:            ## create venv & install deps
	$(PY) -m venv .venv && .venv/bin/pip install -U pip
	.venv/bin/pip install poetry==1.8.2
	.venv/bin/poetry install

compose-up:      ## start local services
	docker compose -f docker/docker-compose.yml up -d

etl-run-once:    ## single threaded local ETL
	.venv/bin/python -m src.etl.be_moniteur
	.venv/bin/python -m src.etl.es_boe
	.venv/bin/python -m src.etl.de_bgbl

serve-local:     ## run API locally
	.venv/bin/uvicorn src.api:create_app --factory --reload

test:            ## run all tests
	.venv/bin/pytest -q
```

---

## 10. TestsÂ `tests/test_e2e.py`Â `REQ:`

* Fixture spins up API in a background thread on portÂ `9000`.
* Execute `etl-run-once`.
* Assert:

  ```python
  assert db.session.query(Document).filter_by(jurisdiction="BE").count() >= 0
  ```

  (countâ€¯â‰¥â€¯0 because today may have no docs; the important part is *no crash*).
* HTTP `GET http://localhost:9000/healthz` â†’ 200.
* `GET /search?q=tax&jurisdiction=ES` â†’ 200 (payload list).

---

## 11. CI Â `.github/workflows/ci.yml`Â `REQ:`

* Runs on `ubuntuâ€‘latest`.
* Steps: checkout, `make init`, `make compose-up`, `make test`.
* Matrix: `{ python-version: [3.12] }`.
* Use Dockerâ€‘layer caching (buildx) to speed up.
* **NoÂ Azure login** in CI â€“ local mode only.

---

## 12. CloudÂ SmokeÂ TestÂ `make smoke-cloud`Â `REQ:`

* Requires envâ€¯vars pointing to real Azure resources.
* Runs a single Function execution via `az functionapp invoke`.
* Confirms that AI Search index has â‰¥â€¯1 document.

---

## 13. CostÂ GuardÂ CheckÂ `scripts/cost_check.sh`Â `REQ:`

* Call `az consumption usage list --top 1` and fail pipeline if projected monthly costÂ >Â â‚¬5.

---

## 14. ðŸŽ¯ AcceptanceÂ Checklist

* [ ] Repo structure matches **exactly**.
* [ ] `make init && make compose-up && make test` passes locally with no external network (except source downloads).
* [ ] `make deploy-infra && make smoke-cloud` works with personal Azure sub.
* [ ] README contains **clear, copyâ€‘pasteâ€‘ready commands** for each step.

---

## 15. Recommended Implementation Order (nonâ€‘binding)

1. **Scaffold repo & poetry project.**
2. **Docker Compose** for Postgres+pgvector and Azurite.
3. **SQLAlchemy models & local search shim.**
4. **Spanish loader** (simplest API).  Run tests.
5. **FastAPI service** with stub search.  Ensure `/healthz` green.
6. **CI pipeline** (local mode).
7. Add **Belgian** XML loader.
8. Add **German** PDF loader (complex; parse only first page for POC).
9. **Azure Bicep** & smoke test.
10. Polish docs, costâ€‘check, OpenAPI generation, PowerÂ Connector guide.

---

### ðŸ”§ Tips

* Use `rich` logging for pretty ETL output in GitHub Actions logs.
* For OCR fallback, cache Tesseract download in Docker layer.
* Limit OpenAI embedding calls to first **4â€¯000Â tokens** per doc chunk to cut cost.
* Wrap every network call in `tenacity.retry` with exponential backâ€‘off.

Happy coding â€“ remember: the bot in CI is ruthless. âœ…
"""
